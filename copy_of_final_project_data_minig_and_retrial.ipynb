{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOT+NWIVJvtIC/MJD6n5XS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NourhanDeifSayed/Search-Engine/blob/main/copy_of_final_project_data_minig_and_retrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file_name = 'cisi.zip'\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall('cisi_dataset')\n",
        "!ls cisi_dataset\n"
      ],
      "metadata": {
        "id": "SQfJrt6SDYIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "IUuK7mbiEkOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cisi_dataset(data_dir):\n",
        "    documents_path = os.path.join(data_dir, 'CISI.ALL')\n",
        "    queries_path = os.path.join(data_dir, 'CISI.QRY')\n",
        "    qrels_path = os.path.join(data_dir, 'CISI.REL')\n",
        "\n",
        "    documents_df = read_documents(documents_path)\n",
        "    queries_df = read_queries(queries_path)\n",
        "    qrels_df = read_qrels(qrels_path)\n",
        "    return documents_df, queries_df, qrels_df\n",
        "\n",
        "# Read documents from CISI.ALL file\n",
        "def read_documents(documents_path):\n",
        "    with open(documents_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    documents = []\n",
        "    current_document = None\n",
        "    for line in lines:\n",
        "        if line.startswith('.I'):\n",
        "            if current_document is not None:\n",
        "                current_document['Text'] = current_document['Text'].split('\\t')[0].strip()  # Remove anything after the first tab\n",
        "                documents.append(current_document)\n",
        "            current_document = {'ID': line.strip().split()[1], 'Text': ''}\n",
        "        elif line.startswith('.T'):\n",
        "            continue\n",
        "        elif line.startswith('.A') or line.startswith('.B') or line.startswith('.W') or line.startswith('.X'):\n",
        "            continue\n",
        "        else:\n",
        "            current_document['Text'] += line.strip() + ' '\n",
        "\n",
        "    # Append the last document\n",
        "    if current_document is not None:\n",
        "        current_document['Text'] = current_document['Text'].split('\\t')[0].strip()  # Remove anything after the first tab\n",
        "        documents.append(current_document)\n",
        "    documents_df = pd.DataFrame(documents)\n",
        "    return documents_df\n",
        "\n",
        "# Read queries from CISI.QRY file\n",
        "def read_queries(queries_path):\n",
        "    with open(queries_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    query_texts = []\n",
        "    query_ids = []\n",
        "    current_query_id = None\n",
        "    current_query_text = []\n",
        "    for line in lines:\n",
        "        if line.startswith('.I'):\n",
        "            if current_query_id is not None:\n",
        "                query_texts.append(' '.join(current_query_text))\n",
        "                current_query_text = []\n",
        "            current_query_id = line.strip().split()[1]\n",
        "            query_ids.append(current_query_id)\n",
        "        elif line.startswith('.W'):\n",
        "            continue\n",
        "        elif line.startswith('.X'):\n",
        "            break\n",
        "        else:\n",
        "            current_query_text.append(line.strip())\n",
        "    # Append the last query\n",
        "    query_texts.append(' '.join(current_query_text))\n",
        "    queries_df = pd.DataFrame({\n",
        "        'qid': query_ids,\n",
        "        'raw_query': query_texts})\n",
        "    return queries_df\n",
        "\n",
        "# Read qrels from CISI.REL file\n",
        "def read_qrels(qrels_path):\n",
        "    qrels_df = pd.read_csv(qrels_path, sep='\\s+', names=['qid','Q0','docno','label'])\n",
        "    return qrels_df"
      ],
      "metadata": {
        "id": "q4IRYXTYD9Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "akmrkD0eyLca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/cisi_dataset'\n",
        "documents_df, queries_df, qrels_df = load_cisi_dataset(data_dir)\n",
        "documents_df['Text'][0]"
      ],
      "metadata": {
        "id": "w08mZDDrEa4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_df"
      ],
      "metadata": {
        "id": "xBDDQUnjEl_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qrels_df"
      ],
      "metadata": {
        "id": "WoPtQlruErR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_df[\"docno\"]=documents_df[\"ID\"].astype(str)\n",
        "documents_df"
      ],
      "metadata": {
        "id": "ZcirfHQPEtte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries_df[\"qid\"]=queries_df[\"qid\"].astype(str)\n",
        "queries_df"
      ],
      "metadata": {
        "id": "UnJBwOL5Ezno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-terrier\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "7L0w0pH3BpXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import pyterrier as pt\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "WvNWfB1QBx2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not pt.started():\n",
        "\n",
        "  pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
      ],
      "metadata": {
        "id": "uvMmw-ISCAed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "_8cfo_FXCDnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n"
      ],
      "metadata": {
        "id": "bvZGTdR7E-KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Steem_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # print (tokens)\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "\n",
        "#a function to clean the tweets\n",
        "def clean(text):\n",
        "   text = re.sub(r\"http\\S+\", \" \", text) # remove urls\n",
        "   text = re.sub(r\"RT \", \" \", text) # remove rt\n",
        "   text = re.sub(r\"@[\\w]*\", \" \", text) # remove handles\n",
        "   text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text) # remove special characters\n",
        "   text = re.sub(r'\\t', ' ', text) # remove tabs\n",
        "   text = re.sub(r'\\n', ' ', text) # remove line jump\n",
        "   text = re.sub(r\"\\s+\", \" \", text) # remove extra white space\n",
        "   text = text.strip()\n",
        "   return text\n",
        "\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words] #Lower is used to normalize al the words make them in lower case\n",
        "    print('Tokens are:',tokens,'\\n')\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "\n",
        "\n",
        "#we need to process the query also as we did for documents\n",
        "def preprocess(sentence):\n",
        "  sentence = remove_stopwords(sentence)\n",
        "  sentence = clean(sentence)\n",
        "  sentence = Steem_text(sentence)\n",
        "\n",
        "  return sentence\n"
      ],
      "metadata": {
        "id": "zs8mx-8GE-Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_df['processed_text'] = documents_df['Text'].apply(preprocess)\n",
        "documents_df"
      ],
      "metadata": {
        "id": "lbJKzys3E-Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries_df[\"query\"]=queries_df[\"raw_query\"].apply(preprocess)\n",
        "queries_df"
      ],
      "metadata": {
        "id": "buxO3I-oFG0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = pt.DFIndexer(\"./DatasetIndex\", overwrite=True)\n",
        "index_ref = indexer.index(documents_df[\"processed_text\"], documents_df[\"docno\"])\n",
        "print(index_ref.toString())"
      ],
      "metadata": {
        "id": "QbbWb998FNng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index_ref.toString())\n",
        "\n",
        "index = pt.IndexFactory.of(index_ref)\n",
        "\n",
        "print(index.getCollectionStatistics().toString())"
      ],
      "metadata": {
        "id": "XtoYXee0FTAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for kv in index.getLexicon():\n",
        "  print(\"%s -> %s \" % (kv.getKey(), kv.getValue().toString()))"
      ],
      "metadata": {
        "id": "NC6PuL0rFZKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "que=\"problems\"\n",
        "que=preprocess(que)"
      ],
      "metadata": {
        "id": "ew1LekLwGETq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    pointer = index.getLexicon()[que]\n",
        "    for posting in index.getInvertedIndex().getPostings(pointer):\n",
        "      print(posting.toString(), \"doclen=%d\" % posting.getDocumentLength())\n",
        "      posting_info = posting.toString()\n",
        "      doc_id = posting_info.split()[0].split(\"(\")[1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "except:\n",
        "    print(\"term not found\")\n"
      ],
      "metadata": {
        "id": "tFQyfbyFGzOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Query=\"I love reading reading books\"\n",
        "Query=preprocess(Query)"
      ],
      "metadata": {
        "id": "ilBeLZdsG47E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_retr = pt.BatchRetrieve(index, controls = {\"wmodel\": \"TF_IDF\"},num_results=15)"
      ],
      "metadata": {
        "id": "-3Sh94TwHEV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=tfidf_retr.search(Query)\n",
        "results"
      ],
      "metadata": {
        "id": "pq0zc1khHH_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_df[['Text']][documents_df['docno'].isin(results['docno'].loc[0:4].tolist())]"
      ],
      "metadata": {
        "id": "XQ71-DPdJSWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git clone https://github.com/terrierteam/terrier-prf/\n",
        "!apt-get install maven\n",
        "%cd /content/terrier-prf/\n",
        "!mvn install\n",
        "!pwd\n",
        "%cd .."
      ],
      "metadata": {
        "id": "FxC6PpK6K8v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rm3_expander = pt.rewrite.RM3(index,fb_terms=10, fb_docs=100)\n",
        "\n",
        "rm3_qe = tfidf_retr>> rm3_expander\n",
        "expanded_query = rm3_qe.search(Query).iloc[0][\"query\"]\n",
        "\n",
        "expanded_query"
      ],
      "metadata": {
        "id": "uTAdL78ILHah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in expanded_query.split()[1:]:\n",
        "  print(s)\n",
        "\n",
        "print(\"\\n\" + Query)"
      ],
      "metadata": {
        "id": "fxSCsPpMLUbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_query_formatted = ' '.join(expanded_query.split()[1:])\n",
        "\n",
        "results_wqe = tfidf_retr.search(expanded_query_formatted)\n",
        "\n",
        "print(\"   Before Expansion    After Expansion\")\n",
        "print(pd.concat([results[['docid','score']][0:5].add_suffix('_1'),\n",
        "            results_wqe[['docid','score']][0:5].add_suffix('_2')], axis=1).fillna(''))\n",
        "\n",
        "#Let's check the tweets text for the top 5 retrieved tweets\n",
        "documents_df[['Text']][documents_df['docno'].isin(results_wqe['docno'].loc[0:5].tolist())]"
      ],
      "metadata": {
        "id": "HWPEEI6hLbau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sentences=documents_df[['Text']][documents_df['docno'].isin(results_wqe['docno'].loc[0:10].tolist())]"
      ],
      "metadata": {
        "id": "XF_mrLiqqkUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "docnos = results_wqe['docno'].loc[0:10].tolist()\n",
        "\n",
        "\n",
        "sentences_list = documents_df.loc[documents_df['docno'].isin(docnos), 'Text'].tolist()\n",
        "\n",
        "\n",
        "print(sentences_list)"
      ],
      "metadata": {
        "id": "HeI8XFokzyoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js('google.colab.kernel.proxyPort(5000)'))"
      ],
      "metadata": {
        "id": "KURqAyuINKRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, send_from_directory\n",
        "import os\n",
        "import pyterrier as pt\n",
        "from datetime import datetime\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "indexer = pt.DFIndexer(\"./DatasetIndex\", overwrite=True)\n",
        "index_ref = indexer.index(documents_df[\"processed_text\"], documents_df[\"docno\"])\n",
        "index = pt.IndexFactory.of(index_ref)\n",
        "\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    html_search = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Search Engine</title>\n",
        "    <link href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css\" rel=\"stylesheet\" id=\"bootstrap-css\">\n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
        "    <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js\"></script>\n",
        "    <script>\n",
        "    $(document).ready(function(){\n",
        "        $('#search_button').click(function(){\n",
        "            var query = $('#search_text').val();\n",
        "            load_data(query);\n",
        "        });\n",
        "        $('#search_text').keyup(function(e){\n",
        "            if(e.which == 13){\n",
        "                var query = $(this).val();\n",
        "                load_data(query);\n",
        "            }\n",
        "        });\n",
        "        function load_data(query) {\n",
        "            $.ajax({\n",
        "                url: \"/search\",\n",
        "                method: \"POST\",\n",
        "                data: {query: query},\n",
        "                success: function(data) {\n",
        "                    $('#result').html(data);\n",
        "                },\n",
        "                error: function(xhr, status, error) {\n",
        "                    console.error('Error:', error);\n",
        "                }\n",
        "            });\n",
        "        }\n",
        "    });\n",
        "    </script>\n",
        "    </head>\n",
        "    <body>\n",
        "    <div class=\"container search-table\">\n",
        "        <p><h2 align=\"center\">Search for anything on your mind</h2></p>\n",
        "        <div class=\"search-box\">\n",
        "            <div class=\"row\">\n",
        "                <div class=\"col-md-3\">\n",
        "                    <h5>Enter Your Queries</h5>\n",
        "                </div>\n",
        "                <div class=\"col-md-6\">\n",
        "                    <input type=\"text\" name=\"search_text\" id=\"search_text\" class=\"form-control\" placeholder=\"Search For Everything here\">\n",
        "                </div>\n",
        "                <div class=\"col-md-3\">\n",
        "                    <button id=\"search_button\" class=\"btn btn-primary\" style=\"background-color: #007bff; border-color: #007bff;\">Search</button>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "        <div id=\"result\"></div>\n",
        "    </div>\n",
        "    <style>\n",
        "    .search-table{\n",
        "        padding: 10%;\n",
        "        margin-top: -6%;\n",
        "    }\n",
        "    .search-box{\n",
        "        background:  #add8e6;\n",
        "        border: 1px solid #ababab;\n",
        "        padding: 3%;\n",
        "    }\n",
        "    .search-box input:focus{\n",
        "        box-shadow:none;\n",
        "        border:2px solid #eeeeee;\n",
        "    }\n",
        "    .search-list{\n",
        "        background: #add8e6;\n",
        "        border: 1px solid #ababab;\n",
        "        border-top: none;\n",
        "    }\n",
        "    .search-list h3{\n",
        "        background: #add8e6;\n",
        "        color: #000080;\n",
        "        padding: 3%;\n",
        "        margin-bottom: 0%;\n",
        "    }\n",
        "    </style>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_search\n",
        "@app.route(\"/search\", methods=['POST'])\n",
        "def search():\n",
        "    try:\n",
        "\n",
        "        if 'query' in request.form:\n",
        "            query = request.form['query']\n",
        "        else:\n",
        "            raise ValueError(\"Query not found in form data.\")\n",
        "\n",
        "        query = preprocess(query)\n",
        "\n",
        "        start_time = datetime.now()\n",
        "        results = tfidf_retr.transform(query)\n",
        "        end_time = datetime.now()\n",
        "\n",
        "        execution_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "\n",
        "        if not results.empty:\n",
        "            results = results.head(6)\n",
        "\n",
        "            relevant_docnos = results['docno'].tolist()\n",
        "            relevant_documents = documents_df[documents_df['docno'].isin(relevant_docnos)]\n",
        "\n",
        "\n",
        "            output_html = \"<h3>Search Results:</h3>\"\n",
        "            output_html += \"<ul>\"\n",
        "            for _, row in results.iterrows():\n",
        "                output_html += \"<li>Document: {}</li>\".format(row['docno'])\n",
        "            output_html += \"</ul>\"\n",
        "\n",
        "            output_html += \"<h3>Relevant Documents:</h3>\"\n",
        "            output_html += \"<ul>\"\n",
        "            for _, doc in relevant_documents.iterrows():\n",
        "                output_html += \"<li>Document: {}, Text: {}</li>\".format(doc['docno'], doc['Text'])\n",
        "            output_html += \"</ul>\"\n",
        "\n",
        "            output_html += \"<p>Execution Time: {:.4f} seconds</p>\".format(execution_time)\n",
        "        else:\n",
        "            output_html = \"<p>No results found.</p>\"\n",
        "    except Exception as e:\n",
        "        output_html = f\"<p>An error occurred: {e}</p>\"\n",
        "    return output_html\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/favicon.ico')\n",
        "def favicon():\n",
        "  return send_from_directory(os.path.join(app.root_path, 'static'), 'favicon.ico', mimetype='image/vnd.microsoft.icon')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "id": "1gnnbuOQL-fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
      ],
      "metadata": {
        "id": "ZpVM2zTULwjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = elmo.signatures[\"default\"](tf.constant(sentences_list))[\"elmo\"]"
      ],
      "metadata": {
        "id": "a5Oe5SudQFW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_1 = embeddings.numpy()[0][7]\n",
        "embedding_2= embeddings.numpy()[1][5]\n",
        "featuresOf_3= embeddings.numpy()[2][8]\n",
        "\n",
        "\n",
        "print(\"Embedding vector embedding_1:\", embedding_1)\n",
        "print(\"Embedding vector embedding_2:\",embedding_2)\n",
        "print(\"Embedding vector featuresOf_3:\",featuresOf_3)"
      ],
      "metadata": {
        "id": "G44MuLv0QF9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "_Wu6AWQswuvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=sentences_list,\n",
        "                 sg=1,\n",
        "                 vector_size=100,\n",
        "                 window=2,\n",
        "                 min_count=1,\n",
        "                 workers=4,\n",
        "                 epochs=20)"
      ],
      "metadata": {
        "id": "GIAXu7tRxDpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = model.wv"
      ],
      "metadata": {
        "id": "OdDTYTJVxM7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = pt.Evaluate(results_wqe,qrels_df,metrics=[\"map\",\"recall\",\"P\"], perquery=True)\n",
        "eval"
      ],
      "metadata": {
        "id": "my6vLXRGPzGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}